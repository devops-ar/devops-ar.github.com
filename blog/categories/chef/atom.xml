<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: chef | devops-ar]]></title>
  <link href="http://devops-ar.github.com/blog/categories/chef/atom.xml" rel="self"/>
  <link href="http://devops-ar.github.com/"/>
  <updated>2013-02-21T08:30:10-03:00</updated>
  <id>http://devops-ar.github.com/</id>
  <author>
    <name><![CDATA[osvaldo]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[instalando servidores con un commit]]></title>
    <link href="http://devops-ar.github.com/blog/2012/11/20/instalando-servidores-con-un-commit/"/>
    <updated>2012-11-20T07:50:00-03:00</updated>
    <id>http://devops-ar.github.com/blog/2012/11/20/instalando-servidores-con-un-commit</id>
    <content type="html"><![CDATA[<h1>Introducción (aka TL;DR)</h1>

<p>Empece a usar chef para deployar los servers (vm) de desarrollo. Partiendo de la forma estandar de uso de chef, llegue al workflow que presento a continuacion.<br/>
Toda la informacion de un server se almacena en un archivo de texto. Configurar un server se reduce a editar el archivo correspondiente y comitear los cambios realizados. <br/>
El workflow se encarga del resto: ejecutar los comandos necesarios, notificar por IRC del estado de la operacion y dejar una traza disponible de los comandos ejecutados para su revision posterior (y/o auditoria/debug/etc).</p>

<!-- more -->


<h2>componentes de chef</h2>

<p>chef tiene tres componentes basicos:</p>

<ul>
<li>chef server</li>
<li>chef workstation</li>
<li>chef client</li>
</ul>


<p>El chef client (node) toma la configuracion del chef server y la aplica.<br/>
La chef workstation es la que instala el chef en los clientes (nodes) y en nuestro caso, invoca la ejecucion del chef client.<br/>
El chef server es donde esta toda la configuracion que se va a aplicar (los cookbooks con las recipes y las data bags). tambien se almacena la informacion de los nodes que son administrador por chef.</p>

<h2>infrastructure as code</h2>

<p>La configuracion que se aplica (cookbooks, databags) se puede guardar en un repositorio de codigo, se decidio utilizar git.  El chef server tiene una interface web, pero no se utiliza, toda la configuracion se hace en los archivos de repositorio y se importa al chef server (mediante el comando knife). De esta manera, todos los cambios estan versionados. Para servidor git se utiliza gitolite (como interface web se recomienda gitlab).</p>

<h1>Workflow</h1>

<p>Se armo un workflow siguiendo la filosofia unix, pequeños componentes que cumplen una tarea en especifico y que pueden ser concatenados para implementar una tarea compleja.<br/>
El sysadmin clona el repo en la estacion de trabajo que vaya a utilizar. Solo se requiere git instalado, su llave ssh publica (para crear el usuario en gitolite) y acceso por ssh al server git.<br/>
Trabaja con los archivos . Hace un commit para guardar los cambios realizados. Por ultimo, hace un push para actualizar el repositorio central.</p>

<p>El repositorio central esta en un gitolite. El gitolite corre bajo un usuario (git) sin privilegios.  <br/>
Se creo un hook <code>post-receive</code> que es el caballo de batalla de este workflow. Cada git push que se realiza contra el repositorio, ejecuta el script de ese hook. <br/>
El script chequea que fue lo que se actualizo y en base a eso toma la accion que corresponde.<br/>
El usuario git tiene configurado el comando knife para poder cargar cookbooks y databags en el chef server.<br/>
Si se actualizo un cookbook, un databag (ver nota siguiente), un role o un environment, se sube (knife upload) al chef server.</p>

<p>Para administrar los servidores se creo un databag: nodes, donde se crea un archivo json (<code>template.json</code>)  por cada servidor que se deployea.</p>

<p><div><script src='https://gist.github.com/4075735.js?file='></script>
<noscript><pre><code>#!/bin/sh

#TODO
# add error checking to knife operations

IRC_URL=&quot;http://irc.example.com/irc.php&quot;
RUNDECK_URL=&quot;https://rundeck.example.com/rundeck/&quot;
template_msg=&quot;modified template:&quot;
node_msg=&quot;modified node:&quot;
envr_msg=&quot;modified environment:&quot;
role_msg=&quot;modified role:&quot;
cookbook_msg=&quot;modified cookbook:&quot;
boot_msg=&quot;boot-node started in rundeck - progress: &quot;
run_msg=&quot;run-client started in rundeck - progres: &quot;
from=&quot;gitolite-chef-server&quot;


# Safety check
if [ -z &quot;$GIT_DIR&quot; ]; then
    echo &quot;Don't run this script from the command line.&quot; &gt;&amp;2
    exit 1
fi

tmpdir=/var/tmp/$$.$RANDOM
mkdir $tmpdir

read oldrev newrev refname
BRANCH=${refname#refs/heads/} 
files=`git diff-tree --stat --summary --find-copies-harder $oldrev..$newrev`
for i in $files
do
  case &quot;$i&quot; in
    data_bags/nodes/*)
      k=`echo $i | cut -f&quot;3&quot; -d&quot;/&quot;`
      j=`basename $k .json`
      if [ $j = &quot;template&quot; ]; then
                    curl  -X POST -d 'json={&quot;msg&quot;:&quot;'${template_msg}' '${j}'&quot;,&quot;from&quot;:&quot;'${from}'&quot;}'  ${IRC_URL}
        exit 0
      fi
      git show HEAD:$i &gt; $tmpdir/$j.json
      knife data bag from file nodes $tmpdir/$j.json
      curl  -X POST -d 'json={&quot;msg&quot;:&quot;'${node_msg}' '${j}'&quot;,&quot;from&quot;:&quot;'${from}'&quot;}'  ${IRC_URL}
      search=`knife node list | grep $j`
      STATUS=$?
      #STATUS=1
      if [ $STATUS -ne 0 ]; then
        # boostrap node
        IP=`sed -n '/eth0/{n; s/^.*target&quot;:&quot;\(.*\)&quot;.*/\1/p}' $tmpdir/$j.json`
        FQDN=`sed -n 's/^.*fqdn&quot;.*&quot;\(.*\)&quot;.*/\1/p' $tmpdir/$j.json`
        ENVIRONMENT=`sed -n 's/^.*environment&quot;.*&quot;\(.*\)&quot;.*/\1/p' $tmpdir/$j.json`
        #echo &quot;the ip: $IP and the env: $ENVIRONMENT&quot;
        out=`ssh chef@rundeck.example.com run -j &quot;boot-node&quot; -p chef-workstation -- -ip ${IP} -fqdn ${FQDN} -env ${ENVIRONMENT}`
        # the url changed from http://rundeck.example.com:4440/
        # to https://rundeck.example.com/rundeck/
        #url=`echo $out | sed 's/.*&lt;\(.*\)&gt;$/\1/'`
        url=`echo $out | sed 's/.*4440\/\(.*\)&gt;$/\1/'`
        url=$RUNDECK_URL$url
        curl  -X POST -d 'json={&quot;msg&quot;:&quot;'${boot_msg}' '${url}'&quot;,&quot;from&quot;:&quot;'${from}'&quot;}'  ${IRC_URL}
        # run-client
        out=`ssh chef@rundeck.example.com run -j &quot;run-chef-client&quot; -p chef-workstation -- -fqdn ${FQDN}`
        url=`echo $out | sed 's/.*4440\/\(.*\)&gt;$/\1/'`
        url=$RUNDECK_URL$url
        curl  -X POST -d 'json={&quot;msg&quot;:&quot;'${run_msg}' '${url}'&quot;,&quot;from&quot;:&quot;'${from}'&quot;}'  ${IRC_URL}
      else
        # run-client
        fqdn=`echo ${search} | xargs knife node show | head -n 1 | awk '{print $3}'`
        out=`ssh chef@rundeck.example.com run -j &quot;run-chef-client&quot; -p chef-workstation -- -fqdn $fqdn`
        url=`echo $out | sed 's/.*4440\/\(.*\)&gt;$/\1/'`
        url=$RUNDECK_URL$url
        curl  -X POST -d 'json={&quot;msg&quot;:&quot;'${run_msg}' '${url}'&quot;,&quot;from&quot;:&quot;'${from}'&quot;}'  ${IRC_URL}
      fi
      ;;
    roles/*)
      k=`echo $i | cut -f&quot;2&quot; -d&quot;/&quot;`
      #knife data bag from file nodes $tmpdir/$j.json
      #curl  -X POST -d 'json={&quot;msg&quot;:&quot;se modifico el node: '${j}'&quot;,&quot;from&quot;:&quot;'${from}'&quot;}'  http://rundeck.example.com/irc.php
      #echo &quot;from roles!&quot;
      j=`basename $k .rb`
      git show HEAD:$i &gt; $tmpdir/$k
      knife role from file $tmpdir/$k
      curl  -X POST -d 'json={&quot;msg&quot;:&quot;'${role_msg}' '${j}'&quot;,&quot;from&quot;:&quot;'${from}'&quot;}'  ${IRC_URL}
      ;;
    environments/*)
      k=`echo $i | cut -f&quot;2&quot; -d&quot;/&quot;`
      j=`basename $k .rb`
      git show HEAD:$i &gt; $tmpdir/$k
      knife environment from file $tmpdir/$k
      curl  -X POST -d 'json={&quot;msg&quot;:&quot;'${envr_msg}' '${j}'&quot;,&quot;from&quot;:&quot;'${from}'&quot;}'  ${IRC_URL}
      ;;
    cookbooks/*)
      k=`echo $i | cut -f&quot;2&quot; -d&quot;/&quot;`
      #echo &quot;knife cookbook upload $k from $tmpdir&quot;
      GIT_WORK_TREE=$tmpdir git checkout -f
      cd $tmpdir
      knife cookbook upload $k
      curl  -X POST -d 'json={&quot;msg&quot;:&quot;'${cookbook_msg}' '${k}'&quot;,&quot;from&quot;:&quot;'${from}'&quot;}'  ${IRC_URL}
      ;;
    */files/*)
      k=`echo $i | cut -f&quot;2&quot; -d&quot;/&quot;`
      curl  -X POST -d 'json={&quot;msg&quot;:&quot;'${cookbook_msg}' '${k}'&quot;,&quot;from&quot;:&quot;'${from}'&quot;}' ${IRC_URL}
      ;;
    */templates/*)
      k=`echo $i | cut -f&quot;2&quot; -d&quot;/&quot;`
      curl  -X POST -d 'json={&quot;msg&quot;:&quot;'${cookbook_msg}' '${k}'&quot;,&quot;from&quot;:&quot;'${from}'&quot;}'  ${IRC_URL}
      ;;
    *)
      # Anything else (is there anything else?)
      #echo &quot;anything else ... $i&quot;
      #exit 1
      ;;
  esac
done
rm -fr $tmpdir
exit 0</code></pre></noscript></div>
</p>

<p>Si el hook detecta que se hizo el push de un node (ya sea porque se creo o porque se actualizo, hablamos del archivo json), se conecta al chef server y revisa si existe ese node.<br/>
Si no existe, le hace un bootstrap al node primero, le asigna el environment y ejecuta el comando chef-client. Si ya existia, directamente invoca el chef-client en el node para aplicar cualquiera fuera la nueva configuracion que se realizo.<br/>
Se crearon recipes que utilizan informacion de este archivo json para configurar el servidor.<br/>
En el archivo json del node se definen items como los usuarios que van a tener acceso por ssh al server o el listado de software que va a tener instalado. <br/>
Si se realizo la modificacion de un cookbook, el hook lo sube al chef-server. Idem para un role, environment u otro data bag que no sea nodes.  <br/>
El hook no tiene ningun acceso a los servidores (nodes). Cuando se requiere ejecutar una accion, lanza un job en una instancia de rundeck. Esta instancia seria el equivalente a la chef workstation.</p>

<h1>rundeck</h1>

<p>En un server con acceso a todas las vm por ssh (jumphost) se creo una cuenta de usuario (chef) sin privilegios. Se configuro el comando knife y la llave ssh privada para acceder a las vm. <br/>
Bajo esta cuenta se invoca una instancia de rundeck. En el rundeck se creo un proyecto con dos jobs: uno para el bootstraping de un node (instalar el cliente de chef) y otro para la ejecucion del chef-client. Se configuro el usuario git para que pudiera lanzar cualquiera de estos jobs por ssh con los argumentos necesarios.<br/>
Cuando el hook necesita ejecutar un comando en un node, crea el job correspondiente en el rundeck, notifica la url del job (el rundeck se la devuelve cuando crea el job) y termina su ejecucion.<br/>
El rundeck almacena las trazas de todo comando que ejecuta. Esto permite hacer un seguimiento en caso de ser necesario.</p>

<h1>Seguridad</h1>

<p>Toda modificación se hace via git, cada commit se guarda con las credenciales de la persona que lo realizó.<br/>
El dev/sysadmin solo requiere tener git instalado y acceso por ssh al repositorio (y tener una cuenta de usuarios con permisos para comitear).<br/>
El server de git no necesita ningun acceso a ningun server, solo la posibilidad de usar el knife para subir informaciones al chef server. Todo cambio queda registrado con el commit de quien lo hizo. El repo funciona bajo un usuario sin privilegios.<br/>
El server de rundeck tambien se ejecuta bajo un usuario sin privilegios. Solo recibe jobs desde el hook del commit.</p>

<h1>Auditoria</h1>

<p>El git mantiene un registro de todos los commits realizados.<br/>
El rundeck mantiene una traza de la ejecucion de todos los comandos realizados.</p>

<h1>IRC</h1>

<p>Para el irc se utiliza irccat como bot que mediante una pagina en php recibe el post y lo envia al irc server.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[deploying to aws]]></title>
    <link href="http://devops-ar.github.com/blog/2012/10/02/deploying-to-aws/"/>
    <updated>2012-10-02T09:31:00-03:00</updated>
    <id>http://devops-ar.github.com/blog/2012/10/02/deploying-to-aws</id>
    <content type="html"><![CDATA[<p>Amazon had done a great job making the cloud accesible to anyone. Tools like Chef (kudos to Opscode) allow us to make deployments in a pretty easy way. The following post will show how to put them together. A flask (could had been a lamp or any other) application is deployed to Amazon EC2 using Chef. As an added bonus, local development using Vagrant is included.</p>

<h2>tl;dr</h2>

<p>For those in a hurry, just clone the <a href="https://github.com/otsuarez/flask-blog">sample flask app code</a> and the <a href="https://github.com/otsuarez/flask2aws">chef cookbooks</a> repositories and start from there.</p>

<!-- more -->


<h1>Objective</h1>

<p>The original goal for this project was to deploy a group of servers in Amazon for a web based application (api rest interface). The architecture was designed with a load balancing web front end, an app server farm behind it and a third layer of database servers. This article only deals with a standalone server installation thus the cookbooks provide an extensible path for implementing the three-tiered application.</p>

<h1>Components</h1>

<h2>App code</h2>

<p>As a sample app, a simple blog using flask: <a href="https://bitbucket.org/r0sk/flask-htmlblog">FlaskBlog</a>  was choosed. No point on developing a full api/rest interface at this point. Code for this app can be found on this <a href="https://github.com/otsuarez/flask-blog">github repo</a>.</p>

<h2>Chef server</h2>

<p>A chef server was installed (details are not included) on a another host but a Hosted Chef Server might had done the job as well.</p>

<h2>Vagrant</h2>

<p>For testing the custom cookbooks, Vagrant was used on a local environment (<a href="https://github.com/otsuarez/flask2aws/blob/master/Vagrantfile">Vagrantfile</a> available).. The whole architecture could be testing this way since Vagrant allows us to create what it is called: "Multi-VM Environments".</p>

<p>Cookbooks from Opscode repository were used (no point in reinventing the wheel). For handling the deploying and configuration, custom cookbooks were written.</p>

<h2>Opscode cookbooks (github.com/cookbooks/)</h2>

<ul>
<li>nginx</li>
<li>database</li>
<li>mysql (database cookbook dependency)</li>
<li>openssl (mysql cookbook dependency)</li>
</ul>


<h2>Custom cookbooks</h2>

<ul>
<li>appstack (takes care of code deployment, from the code repository to the proper directory on the filesystem)</li>
<li>webstack (handles all the configurations required for the app to work to be done across the whole app stack)</li>
<li>uwsgi (webstack cookbook dependency)</li>
</ul>


<p>The uwsgi cookbook only installs the uwsgi package. Since this package could be used by other cookbooks, it was created as an standalone cookbook.</p>

<h2>Deployment</h2>

<p>The appstack cookbook handles the deployment of the code. It creates the required directories and downloads the code from the repository. Github is used but any other could be used. Included keys (deploy) are only added for information purposes, those are not working ones.</p>

<p>Upon deployment, the webstack creates a database, importing the file "db/initial.sql" from the app code directory. The webstack cookbook creates the config.py file, which defines the database connection string for the app on the app code directory. It also creates the nginx virtual host file with the configuration for using the uwsgi service as the backend. The uwsgi.ini.erb template creates the corresponding uwsgi aplication file for the uwsgi service daemon to serve the app code.</p>

<h1>Chef workstation</h1>

<p>The whole setup requires 3 set of authentication credentials.</p>

<ul>
<li>github epo access</li>
<li>chef server</li>
<li>amazon aws</li>
</ul>


<h2>github</h2>

<p>The application code is stored on a github repository. A new (not any personal one) ssh key pair was created. The public key was added as a deploy key to the repo on github. The private key was added to the application deployment cookbook (appstack).</p>

<h2>chef server</h2>

<p>Using the chef server requires a client pem certificate which can be obtained via the Web console, creating a new client. That will allows you to work with the chef server from a workstation, using the knife command. For bootstrapping nodes, another credential is required, the validation.pem certificate. This one can be found on the /etc/chef/ directory of the chef server host filesystem.</p>

<h2>amazon ec2</h2>

<p>There're two levels of access here. One is related to the ssh key used to log into the servers created on amazon. This key will be stored usually on the ~/.ssh directory of the workstation.</p>

<p>Create your SSH key pair for EC2 (at https://console.aws.amazon.com/ec2/home?#) and save your identity file on the local workstation (will be asumming you named the downloaded file acmeec2.pem)</p>

<p><code>sh
cd ~/.ssh
mv ~/Downloads/acmeec2.pem .
chmod 400 acmeec2.pem
ssh-add ~/.ssh/acmeec2.pem
</code></p>

<p>The other set of credentials are the ones used by the knife command for interacting with the AWS infrastructure. They are being used in the .chef/knife.rb file as env vars.
You need to add them to your ~/.bashrc or ~/.zshrc file.</p>

<p>```sh</p>

<h1>EC2</h1>

<p>export acme_aws_access_key_id="AZIAIGAI7VRAOZEBNAWA"
export acme_aws_secret_access_key="ABJVqDRQm145Gmz+ee09MkqvINIuHs/wjEzaqdsN"
```</p>

<h2>Working with the knife command</h2>

<p>creating the server</p>

<p><code>sh
knife ec2 server create "role[webstack]" -I ami-3962a950  -x ubuntu --region eu-west-1
</code></p>

<p>Checking the new server had been created and successfully added to the chef server.</p>

<p><code>sh
osvaldo@local:~/src/acme/flask2aws$ knife ec2 server list
Instance ID      Public IP        Private IP       Flavor           Image            SSH Key          Security Groups  State          
i-1f14ab82       174.129.142.100  10.28.59.8    m1.small         ami-3962a950     acmeec2    default          running        
osvaldo@local:~/src/acme/flask2aws$ knife node list
  ip-10-28-59-8.ec2.internal
  localhost
osvaldo@local:~/src/acme/flask2aws$
osvaldo@otoja-lm:~/src/acme/flask2aws$ knife node show ip-10-28-59-8.ec2.internal
Node Name:   ip-10-28-59-8.ec2.internal
Environment: _default
FQDN:        ip-10-28-59-8.ec2.internal
IP:          174.129.142.100
Run List:    role[webstack]
Roles:       webstack
Recipes:     webstack
Platform:    ubuntu 11.10
osvaldo@otoja-lm:~/src/acme/flask2aws$
</code>
et voila!</p>

<p>Testing ssh connection to the amazon server and sudo privileges.</p>

<p>```sh
osvaldo@local:~/src/acme/flask2aws$  ssh ubuntu@174.129.142.100 -i ~/.ssh/acmeec2.pem
Welcome to Ubuntu 11.10 (GNU/Linux 3.0.0-13-virtual i686)</p>

<ul>
<li>Documentation:  https://help.ubuntu.com/</li>
</ul>


<p>  System information as of Fri May 11 20:36:35 UTC 2012</p>

<p>  System load:  0.0              Processes:           53
  Usage of /:   8.9% of 9.84GB   Users logged in:     0
  Memory usage: 10%              IP address for eth0: 10.28.59.8
  Swap usage:   0%</p>

<p>  Graph this data and manage this system at https://landscape.canonical.com/
New release '12.04 LTS' available.
Run 'do-release-upgrade' to upgrade to it.</p>

<p>Get cloud support with Ubuntu Advantage Cloud Guest
  http://www.ubuntu.com/business/services/cloud
ubuntu@ip-10-28-59-8:~$ sudo su -
root@ip-10-28-59-8:~#
```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[sendmail cookbook]]></title>
    <link href="http://devops-ar.github.com/blog/2012/05/19/sendmail-cookbook/"/>
    <updated>2012-05-19T14:02:00-03:00</updated>
    <id>http://devops-ar.github.com/blog/2012/05/19/sendmail-cookbook</id>
    <content type="html"><![CDATA[<p>Postfix se ha convertido en el MTA por excelencia para la mayoría de las distribuciones Linux. Sin embargo, algunas, como CentOS (hasta la 5 al menos) instalan sendmail de manera predeterminada. A menos que estemos instalando un servidor de mensajeria, utilizar sendmail sería la mejor opción. Es sencillo de configurar, ya esta instalado y para el uso que le vamos a dar, cumple su cometido.</p>

<p>Para cubrir la parte de configuración y ahorrarnos ese paso, se armo un cookbook que solo contiene tres archivos.</p>

<p>Lo primero que hacemos es crear un archivo de atributos donde se define el smarthost que se este utilizando. Como todo atributo, puede ser modificado luego a nivel de role o de ambiente (environment), incluso, a nivel de node mediante el uso de un data bag.</p>

<p><code>rb cookbooks/sendmail/attributes/default.rb
default["sendmail"]["smarthost"] = "mail-gateway.acme.com"
</code></p>

<p>Sendmail maneja sus archivos de configuración a través de macros m4. Nunca se editan los archivos .cf tal como se aclaran en los mismos:</p>

<p>```sh sendmail.cf line 25</p>

<h5>DO NOT EDIT THIS FILE!  Only edit the source .mc file.</h5>

<p>```</p>

<p>El archivo de template de sendmail.mc es una copia del archivo original, solo se adiciona la linea de definición del smarthost utilizando el atributo antes definido.</p>

<p><code>rb cookbooks/sendmail/templates/default/sendmail.mc.erb
define(`SMART_HOST', `&lt;%= node['sendmail']['smarthost'] %&gt;')dnl
</code></p>

<p>El tercer archivo es la receta como tal.</p>

<p>```rb cookbooks/sendmail/recipes/default.rb
package "m4"
package "sendmail"
package "sendmail-cf"</p>

<p>service 'sendmail'</p>

<p>template "/etc/mail/sendmail.mc" do
  source "sendmail.mc.erb"
  mode 0644
  owner "root"
  group "root"
  notifies :run, "execute[create_sendmailcf_file]", :immediately
  notifies :restart, "service[sendmail]"
end</p>

<p>execute "create_sendmailcf_file" do
  command "/usr/bin/m4 /etc/mail/sendmail.mc > /etc/mail/sendmail.cf"
  action :nothing
end
```</p>

<p>Las tres primeras lineas se aseguran de que los paquetes requeridos se encuentren instalados en el sistema.</p>

<p>La declaración del servicio <code>sendmail</code> se encarga de configurarlo para que inicie de manera automática y poder enviarle notificaciones desde otras partes de la receta.</p>

<p>Se crea el archivo de configuración del sendmail: sendmail.mc a partir del template. En el caso de que ya existiera no se hace mas nada. Para los casos en que se crea o fue modificado, se ejecuta de manera inmediata el comando de creación del archivo sendmail.cf y se declara que el servicio de sendmail sea reiniciado, para tomar los cambios aplicados, una vez que el chef haya terminado de aplicar todas las recetas.</p>

<p>El comando <code>create_sendmailcf_file</code> no hace nada de manera predeterminada. Solo se ejecuta en caso de modificación del template del archivo de configuración del sendmail.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[empezando con chef]]></title>
    <link href="http://devops-ar.github.com/blog/2012/02/03/empezando-con-chef/"/>
    <updated>2012-02-03T02:20:00-03:00</updated>
    <id>http://devops-ar.github.com/blog/2012/02/03/empezando-con-chef</id>
    <content type="html"><![CDATA[<h1>Componentes de una instalación de Chef</h1>

<p>Una instalación de Chef va a contar típicamente con tres componentes.</p>

<ul>
<li><strong>Chef Server</strong> Es el host donde esta la base de datos de los nodes registrados, los certificados de las credenciales de los mismos y los cookbooks.</li>
<li><strong>Chef Client</strong> Es el node gestionado a través del Chef Server. Una vez hecho el bootstraping, cada vez que se ejecute el cliente de chef (comando chef-client) se conecta al Chef Server, descarga y aplica la configuración establecida por las recetas, roles y environments que tenga asignado.</li>
<li><strong>Workstation</strong> Puede o no ser un node de chef, pero si va tener las credenciales de un usuario registrado con el Chef Server. Desde este equipo se ejecutan comandos de knife y se trabajan con los cookbooks.</li>
</ul>


<h1>Bootstraping</h1>

<p>La instalación de chef en un host (node) para que pueda ser gestionado por un chef server se llama bootstraping. Se realiza con el comando <a href="http://wiki.opscode.com/display/chef/Knife">knife</a> y el subcomando <a href="http://wiki.opscode.com/display/chef/Knife+Bootstrap">bootstrap</a>.</p>

<p>Por ejemplo:</p>

<p><code>sh
knife bootstrap 10.24.2.51 -x osvaldo --sudo  -N unixtools.tx.corp.acme.com -r 'role[demo]' -d rhel5-demo
</code></p>

<p>Con ese comando, se esta ejecutando el comando knife, el subcomando bootstrap, la dirección IP del server que se le va a hacer el bootstrap y los parámetros, que entonces, serán del subcomando.</p>

<pre><code>--sudo:
    El bootstrap se ejecuta via sudo.
-x, --ssh-user USERNAME:
    El username (nombre de usuario) con el que se realiza la conexión ssh.
-N, --node-name NAME:
    El nombre con el que el host va a ser registrado (como un node) en el server Chef. Usamos el fqdn (el node name tiene que ser único).
-r, --run-list RUN_LIST:
    Un listado separado por comas de los roles o recetas que se van a aplicar en el momento del bootstrapping.
-d, --distro DISTRO:
    El template que se va a utilizar para el bootstrap.
</code></pre>

<p>El bootstraping se debe realizar desde la workstation. Los templates del bootstrap son scripts en ruby pero con codigo en bash y se colocan en alguna de las locaciones predeterminadas.</p>

<p><code>sh  tree ~/chef-repo/.chef
|-- bootstrap
|   `-- glb-rhel5.erb
|-- knife.rb
|-- osvaldo.pem
`-- validation.pem
</code></p>

<p>Una locación común seria ~/chef-repo/.chef. En esta carpeta tenemos:</p>

<ul>
<li><strong>validation.pem</strong>: Es el certificado que utilizan los nodes para validarse ante el Chef Server</li>
<li><strong>osvaldo.pem</strong>: Certificado del cliente. Lo utiliza el operador para trabajar desde ese host como workstation.</li>
<li><strong>knife.rb</strong>: Archivo de configuración del comando knife.</li>
<li><strong>bootstrap</strong>: Directorio donde se almacenan los templates para el bootstrap.</li>
<li><strong>glb-rhel5.rb</strong>: Template para la instalación de un servidor RedHat (CentOS) de ACME.</li>
</ul>


<p>```rb bootstrap/glb-rhel5.erb
bash -c '</p>

<p>echo &lt;%= @config[:chef_node_name] %> > /tmp/chef_node_name</p>

<p>PROPERHOSTNAME=<code>cat /tmp/chef_node_name | cut -d. -f1</code>
PROPERDNSDOMAINNAME=<code>cat /tmp/chef_node_name | sed s/^$PROPERHOSTNAME.//</code></p>

<p>IPV4ADDR=<code>/sbin/ip addr | grep eth0 | grep "inet " | cut -d " " -f 6 | cut -d \/ -f 1</code></p>

<p>cp /etc/hosts /etc/hosts.chef-bck</p>

<p>NETCONF="/etc/sysconfig/network"
grep "<sup>HOSTNAME=${PROPERHOSTNAME}$"</sup> ${NETCONF} || ( sed -i s/<sup>HOSTNAME.*$//</sup> ${NETCONF} &amp;&amp; echo "HOSTNAME=${PROPERHOSTNAME}" >> ${NETCONF})</p>

<p>(
cat &lt;<EOF
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
${IPV4ADDR} ${PROPERHOSTNAME}.${PROPERDNSDOMAINNAME} ${PROPERHOSTNAME}
EOF
) > /etc/hosts</p>

<p>hostname $PROPERHOSTNAME</p>

<p>if [ ! -d /usr/local/rvm ]; then</p>

<pre><code>wget http://192.168.20.50/rvm-chef-0.10.4-r0.noarch.rpm -P /tmp
rpm -ivh /tmp/rvm-chef-0.10.4-r0.noarch.rpm
</code></pre>

<p>fi</p>

<p>(
cat &lt;&lt;'EOP'
[[ -s "/usr/local/rvm/scripts/rvm" ]] &amp;&amp; . "/usr/local/rvm/scripts/rvm"
EOP
) >> .bashrc
[[ -s "/usr/local/rvm/scripts/rvm" ]] &amp;&amp; . "/usr/local/rvm/scripts/rvm"
rvm use --default ruby-1.9.2</p>

<p>mkdir -p /etc/chef</p>

<p>(
cat &lt;&lt;'EOP'
&lt;%= validation_key %>
EOP
) > /tmp/validation.pem
awk NF /tmp/validation.pem > /etc/chef/validation.pem
rm /tmp/validation.pem</p>

<p>(
cat &lt;&lt;'EOP'
&lt;%= config_content %>
EOP
) > /etc/chef/client.rb</p>

<p>(
cat &lt;&lt;'EOP'
&lt;%= { "run_list" => @run_list }.to_json %>
EOP
) > /etc/chef/first-boot.json</p>

<p>chef-client -j /etc/chef/first-boot.json'
```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Herramientas de Chef]]></title>
    <link href="http://devops-ar.github.com/blog/2011/12/17/chef-tools/"/>
    <updated>2011-12-17T14:07:00-03:00</updated>
    <id>http://devops-ar.github.com/blog/2011/12/17/chef-tools</id>
    <content type="html"><![CDATA[<p>Este artículo se puede visualizar en formato de presentación <a href="/talks/chef/chef-tools.html">aquí</a>.</p>

<h2><a href="http://wiki.opscode.com/display/chef/Ohai">ohai</a></h2>

<p>Ohai detecta información acerca del sistema operativo. Puede ser utilizado como un comando independiente pero su función primaria consiste en proveer información sobre el Nodo a Chef.
Cuando se ejecuta, recolecta información detallada y extensible acerca del equipo sobre el esta corriendo, como por ejemplo, configuración de Chef, el hostname, el FQDN, la configuración de red, así como datos sobre la memoria, la CPU, la plataforma y el kernel.</p>

<p>Cuando se ejecuta independiente, Ohai imprime un bloque de datos en formato JSON con toda la información que puede obtener del sistema. Cuando se utiliza a través de Chef, ese bloque de datos JSON es enviado al servidor de Chef como atributos "automáticos" del Nodo para actualizar la información del mismo que pudiera existir previamente.</p>

<h2><a href="http://wiki.opscode.com/display/chef/Knife">knife</a></h2>

<p>Knife es una poderosa herramienta de linea de comandos (CLI) que forma parte de Chef.
Es utilizada por los administradores para interactuar con la API del Servidor de Chef y con el Repositorio local de Chef. Permite manipular los Nodos, Cookbooks, Roles, Databags, Ambientes, etc. Además, puede ser utilizada para la propia instalación de Chef en nuevos sistemas.</p>

<h2>Ohai</h2>

<p>```sh
osvaldo@vostro:~$ ohai
{
  "languages": {</p>

<pre><code>"perl": {
  "version": "5.10.1",
  "archname": "i486-linux-gnu-thread-multi"
},
"python": {
  "version": "2.6.6",
  "builddate": "Dec 27 2010, 00:02:40"
},
</code></pre>

<p>...
  "kernel": {</p>

<pre><code>"name": "Linux",
"release": "2.6.32-5-686",
"version": "#1 SMP Thu Nov 3 04:23:54 UTC 2011",
"machine": "i686",
</code></pre>

<p>...
  "lsb": {</p>

<pre><code>"id": "Debian",
"description": "Debian GNU/Linux 6.0.3 (squeeze)",
"release": "6.0.3",
"codename": "squeeze"
</code></pre>

<p>  },
  "platform": "debian",
  "platform_version": "6.0.3",
...
```</p>

<h2>Knife :: the killer app</h2>

<p>Que pasa si queremos obtener una mirada del estado de los servidores que pertenecen a un rol en particular?</p>

<p><strong>Sort by 5min load average</strong></p>

<p><code>sh
[osvaldo@coldplay ~]$ knife ssh -x otoja 'roles:demo'  'cat /proc/loadavg | awk "{print $2}"' | sort -n -k 2
nirvana.example.com 0.00 0.00 0.00 1/149 16941
stp.example.com 0.00 0.00 0.00 4/182 25165
soundgarden.example.com  0.40 0.10 0.03 1/179 485
</code></p>

<p><strong>Sort by uptime</strong></p>

<p><code>sh
[osvaldo@coldplay ~]$ knife ssh -x otoja 'roles:demo' 'uptime' | sort -n -k 4
nirvana.example.com  21:25:39 up 8 days,  6:30,  2 users,  load average: 0.00, 0.00, 0.00
soundgarden.example.com   21:24:04 up 8 days,  9:00,  1 user,  load average: 0.20, 0.08, 0.02
stp.example.com  21:25:39 up 33 days, 10:47,  2 users,  load average: 0.00, 0.00, 0.00
</code></p>

<p><strong>puedo hacer un tail a un archivo al mismo tiempo en todos los servidores con solo un comando?</strong></p>

<p><code>sh
knife ssh -x otoja 'roles:demo' 'sudo tail -f /var/log/messages'
</code>
y ahora a mirar la pantalla ...</p>

<p><strong>que servidores estan utilizando python 2.*?</strong></p>

<p><code>sh
[osvaldo@coldplay ~]$ knife exec -E "search(:node, 'languages_python_version:2.*'  ).each {|host|  puts host[:fqdn] + ' python version: ' + host[:languages][:python][:version] }"
soundgarden.example.com python version: 2.4.3
nirvana.example.com python version: 2.4.3
stp.example.com python version: 2.4.3
vostro python version: 2.6.6
</code></p>

<p><strong>cual es el estado de un servicio X (ntp en el ejemplo) en un grupo determinado de servidores (con el role demo)?</strong></p>

<p><code>
[osvaldo@coldplay ~]$ knife ssh -x otoja -m "`knife exec -E "search(:node, 'roles:demo').each {|host| puts host[:fqdn] }" | xargs`" '/etc/init.d/ntpd status'
stp.example.com ntpd is stopped
soundgarden.example.com  ntpd is stopped
nirvana.example.com ntpd is stopped
</code></p>

<p><strong>que servidores tienen instalado java version 1.*?</strong></p>

<p><code>sh
[osvaldo@coldplay ~]$ knife exec -E "search(:node, 'languages_java_version:1.*'  ).each {|host|  puts host[:fqdn] + ' java version: ' + host[:languages][:java][:version] }"
toja-lm java version: 1.6.0_18
</code></p>
]]></content>
  </entry>
  
</feed>
